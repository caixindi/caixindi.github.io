<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>《神经网络与深度学习》第4-5章 - Cindy</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#f7f7f7"><meta name="application-name" content="Cindy Page"><meta name="msapplication-TileImage" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/favicon.svg"><meta name="msapplication-TileColor" content="#f7f7f7"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Cindy Page"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="如何理解反向传播？​    首先介绍一下什么是梯度下降法。以神经网络为例，给定一组神经网络参数，即$$\theta&amp;#x3D;{w_1,w_2,…,b_1,b_2,…}$$记$$\nabla L(\theta)&amp;#x3D;\left[\begin{matrix}\frac{\partial L(\theta)}{\partial w_1} \\frac{\partial L(\theta)}{\p"><meta property="og:type" content="blog"><meta property="og:title" content="《神经网络与深度学习》第4-5章"><meta property="og:url" content="http://caixindi.github.io/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC4-5%E7%AB%A0/"><meta property="og:site_name" content="Cindy"><meta property="og:description" content="如何理解反向传播？​    首先介绍一下什么是梯度下降法。以神经网络为例，给定一组神经网络参数，即$$\theta&amp;#x3D;{w_1,w_2,…,b_1,b_2,…}$$记$$\nabla L(\theta)&amp;#x3D;\left[\begin{matrix}\frac{\partial L(\theta)}{\partial w_1} \\frac{\partial L(\theta)}{\p"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%991.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E1.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E2.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E3.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E4.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E5.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E6.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/expression%20swell.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0f%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E7%BB%84%E6%88%90f%E7%9A%84%E5%85%AD%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%87%BD%E6%95%B0.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/f%E5%85%B3%E4%BA%8Ew%E5%92%8Cb%E7%9A%84%E5%AF%BC%E6%95%B0.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%AF%BC%E6%95%B0f%E4%BB%A3%E5%85%A5%E5%80%BC.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%89%8D%E5%90%91%E6%A8%A1%E5%BC%8F%E5%92%8C%E5%8F%8D%E5%90%91%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AF%B9%E6%AF%94.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%861.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%862.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%863.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%864.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%865.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%866.png"><meta property="og:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%867.png"><meta property="article:published_time" content="2021-08-09T16:00:00.000Z"><meta property="article:author" content="Cindy"><meta property="article:tag" content="神经网络与深度学习"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://caixindi.github.io/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC4-5%E7%AB%A0/"},"headline":"《神经网络与深度学习》第4-5章","image":["https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%991.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E1.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E2.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E3.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E4.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E5.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E6.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/expression%20swell.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0f%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E7%BB%84%E6%88%90f%E7%9A%84%E5%85%AD%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%87%BD%E6%95%B0.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/f%E5%85%B3%E4%BA%8Ew%E5%92%8Cb%E7%9A%84%E5%AF%BC%E6%95%B0.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%AF%BC%E6%95%B0f%E4%BB%A3%E5%85%A5%E5%80%BC.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%89%8D%E5%90%91%E6%A8%A1%E5%BC%8F%E5%92%8C%E5%8F%8D%E5%90%91%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AF%B9%E6%AF%94.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%861.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%862.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%863.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%864.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%865.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%866.png","https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%867.png"],"datePublished":"2021-08-09T16:00:00.000Z","author":{"@type":"Person","name":"Cindy"},"publisher":{"@type":"Organization","name":"Cindy","logo":{"@type":"ImageObject","url":"https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/cindy.svg"}},"description":"如何理解反向传播？​    首先介绍一下什么是梯度下降法。以神经网络为例，给定一组神经网络参数，即$$\\theta&#x3D;{w_1,w_2,…,b_1,b_2,…}$$记$$\\nabla L(\\theta)&#x3D;\\left[\\begin{matrix}\\frac{\\partial L(\\theta)}{\\partial w_1} \\\\frac{\\partial L(\\theta)}{\\p"}</script><link rel="canonical" href="http://caixindi.github.io/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC4-5%E7%AB%A0/"><link rel="icon" href="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-72437521-5" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-72437521-5');</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/cindy.svg" alt="Cindy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Discuss on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus/discussions"><i class="fas fa-comments"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-08-09T16:00:00.000Z" title="8/10/2021, 12:00:00 AM">2021-08-10</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络与深度学习</a></span><span class="level-item">25 分钟读完 (大约3698个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">《神经网络与深度学习》第4-5章</h1><div class="content"><h5 id="如何理解反向传播？"><a href="#如何理解反向传播？" class="headerlink" title="如何理解反向传播？"></a>如何理解反向传播？</h5><p>​    首先介绍一下什么是梯度下降法。以神经网络为例，给定一组神经网络参数，即<br>$$<br>\theta&#x3D;{w_1,w_2,…,b_1,b_2,…}<br>$$<br>记<br>$$<br>\nabla L(\theta)&#x3D;\left[\begin{matrix}<br>\frac{\partial L(\theta)}{\partial w_1} \<br>\frac{\partial L(\theta)}{\partial w_2} \<br>\vdots \<br>\frac{\partial L(\theta)}{\partial b_1} \<br>\frac{\partial L(\theta)}{\partial b_2} \<br>\vdots<br>\end{matrix}\right]<br>$$<br>通过计算$\nabla L(\theta^0)$，得到$\theta^1&#x3D;\theta^0-\eta\nabla L(\theta^0)$，通过计算$\nabla L(\theta^1)$，得到$\theta^2&#x3D;\theta^1-\eta\nabla L(\theta^1)$，不断反复，最后找到最佳参数。与线性回归，逻辑回归不同的是，神经网络中往往参数量是百万级别的，如何有效率地计算梯度，这便是反向传播的作用。</p>
<span id="more"></span>

<p>​    这里先介绍一下链式法则，如书中附录部分：</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99.png"></p>
<p>以B.16为例，$\Delta x\rightarrow\Delta y\rightarrow\Delta z$，于是我们有$\frac{\partial z}{\partial x}&#x3D;\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}$。</p>
<p>还有一种情况就是$x&#x3D;g(t),y&#x3D;h(t),z&#x3D;k(x,y)$，此时$$\frac{\partial z}{\partial t}&#x3D;\frac{\partial z}{\partial x}\frac{\partial x}{\partial t}+\frac{\partial z}{\partial y}\frac{\partial y}{\partial t}$$</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%991.png"></p>
<p>这里为了说明方便，我们截取特定输入样本的某一层的某个神经元进行说明。这里只介绍$\frac{\partial C}{\partial w}$，因为计算$\frac{\partial C}{\partial b}$的原理是一样的。<br>$$<br>L(\theta)&#x3D;\sum_{n&#x3D;1}^NC^n(\theta)\rightarrow \frac{\partial L(\theta)}{\partial w}&#x3D;\sum_{n&#x3D;1}^{N}\frac{\partial C(\theta)}{\partial w}<br>$$<br><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E1.png" style="zoom:50%;" /></p>
<p>显然前面的这一项$\frac{\partial z}{\partial w}$非常容易计算，如图中所示，$$\frac{\partial z}{\partial w_1}&#x3D;x_1 , \frac{\partial z}{\partial w_2}&#x3D;x_2$$，<strong>其结果就是连接这个权重的输入</strong>，我们称这个过程为顺推法。而计算$\frac{\partial C}{\partial z}$就比较复杂了，如下图所示：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E2.png" style="zoom:50%;" />

<p>我们知道$\frac{\partial a}{\partial z}&#x3D;\sigma’(z)$，接下来就是计算$\frac{\partial C}{\partial a}$，图中通过链式法则我们可以得到，复杂的情况下，下面可能会产生很多项：<br>$$<br>\frac{\partial C}{\partial a}&#x3D;\frac{\partial z’}{\partial a}\frac{\partial C}{\partial z’}+\frac{\partial z’’}{\partial a}\frac{\partial C}{\partial z’’}<br>$$<br>其中的$\frac{\partial z’}{\partial a}$和$\frac{\partial z’’}{\partial a}$很容易计算，就是$w_3$和$w_4$，此时我们得到：<br>$$<br>\frac{\partial C}{\partial z}&#x3D;\sigma’(z)[w_3 \frac{\partial C}{\partial z’}+w_4\frac{\partial C}{\partial z’’}]<br>$$<br>上述公式可以看出如下的类似的反向神经网络，其中$\sigma’(z)$是一个常量，因为在前向传播中，z的值已经被计算出来，带入即可得到其值。</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E3.png" style="zoom:50%;" />

<p>现在所有的问题都集中在如何计算$\frac{\partial C}{\partial z’}$和$\frac{\partial C}{\partial z’’}$，会遇到两种情况，第一种就是$z’,z’’$已经到达输出层，如图所示：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E4.png" style="zoom:50%;" />

<p>这两项都可以直接计算得出，因为你知道最后的激活函数，你也知道你的损失函数是怎么定义的。</p>
<p>还有一种情况就是$z’,z’’$只是在中间的隐藏层，后面还有许多层，如图所示：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E5.png" style="zoom:50%;" />

<p>我们发现计算$\frac{\partial C}{\partial z’}$和$\frac{\partial C}{\partial z’’}$与之前计算$\frac{\partial C}{\partial z}$一样，例如:<br>$$<br>\frac{\partial C}{\partial z’}&#x3D;\frac{\partial a’}{\partial z’}\frac{\partial C}{\partial a’}&#x3D;\sigma’(z’)[\frac{\partial z_a}{\partial a’}\frac{\partial C}{\partial z_a}+\frac{\partial z_b}{\partial a’}\frac{\partial C}{\partial z_b}]&#x3D;\sigma’(z’)[w_5 \frac{\partial C}{\partial z_a}+w_6\frac{\partial C}{\partial z_b}]<br>$$<br>接下来就是无线套娃的过程，这里我们如果一直从前往后推这是非常复杂的过程，但如果我们一开始就从后往前计算，整个过程就变得很简单，这便是反向传播。如图所示，如果我们要计算$\frac{\partial C}{\partial z_1}$，正向计算我们需要算$\frac{\partial C}{\partial z_3},\frac{\partial C}{\partial z_4},\frac{\partial C}{\partial z_5},\frac{\partial C}{\partial z_6}$，而想要得到$\frac{\partial C}{\partial z_3}$，我们需要计算$\frac{\partial C}{\partial z_5},\frac{\partial C}{\partial z_6}$，以此类推，图中所给示例还是比较简单的结构，如果复杂一点，计算量可想而知。</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%B4%E6%98%8E6.png" style="zoom:50%;" />

<p>而现在，我们从后面往前算，先计算$\frac{\partial C}{\partial z_5},\frac{\partial C}{\partial z_6}$，再根据$\frac{\partial C}{\partial z_5},\frac{\partial C}{\partial z_6}$计算$\frac{\partial C}{\partial z_3},\frac{\partial C}{\partial z_4}$，再根据$\frac{\partial C}{\partial z_3},\frac{\partial C}{\partial z_4}$，计算$\frac{\partial C}{\partial z_1},\frac{\partial C}{\partial z_2}$.</p>
<p>总结一下反向传播的过程就是先顺推出例如$\frac{\partial z}{\partial w}$的值，然后逆推出$\frac{\partial C}{\partial z}$,最终便可以得到$\frac{\partial C}{\partial w}&#x3D;\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}$.</p>
<h5 id="自动梯度计算"><a href="#自动梯度计算" class="headerlink" title="自动梯度计算"></a>自动梯度计算</h5><p>这个部分可以参考论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1502.05767.pdf">《Automatic Differentiation in Machine Learning: a Survey》</a></p>
<ul>
<li><p>数值微分</p>
<p>数值微分（Numerical Differentiation）是用数值方法来计算函数𝑓(𝑥)的导数。函数𝑓(𝑥)的点𝑥的导数定义为<br>$$<br>f’(x)&#x3D;\lim_{\Delta x\rightarrow0}\frac{f(x+\Delta x)-f(x)}{\Delta x}<br>$$<br>找到一个合适的扰动 Δ𝑥十分困难。如果 Δ𝑥 过小，会引起数值计算问题，比如舍入误差；如果Δ𝑥 过大，会增加截断误差，使得导数计算不准确。因此，数值微分的实用性比较差。</p>
<p>何为截断误差？例如函数$e^x$可以展开为无穷幂级数：<br>$$<br>e^x&#x3D;1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\dots<br>$$<br>若取其中的部分项比如:<br>$$<br>e^x\approx 1+x+\frac{x^2}{2!}<br>$$<br>作为其近似计算公式，于是后面的项便舍弃了，便产生了误差，这就是<strong>截断误差</strong>。</p>
<p>书中说使用下面公式来计算梯度，可以减少截断误差，也称中心差分：<br>$$<br>f’(x)&#x3D;\lim_{\Delta x\rightarrow0}\frac{f(x+\Delta x)-f(x-\Delta x)}{2\Delta x}<br>$$<br>利用python程序进行验证：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90.png" style="zoom:50%;" />

<table>
<thead>
<tr>
<th>$f(x)&#x3D;e^ x$</th>
<th>h&#x3D;0.1</th>
<th>h&#x3D;0.01</th>
<th>h&#x3D;0.001</th>
</tr>
</thead>
<tbody><tr>
<td>前向差分</td>
<td>1.0517091807564771</td>
<td>1.005016708416795</td>
<td>1.0005001667083846</td>
</tr>
<tr>
<td>后向差分</td>
<td>0.9516258196404048</td>
<td>0.9950166250831893</td>
<td>0.9995001666249781</td>
</tr>
<tr>
<td>中心差分</td>
<td>1.001667500198441</td>
<td>1.0000166667499921</td>
<td>1.0000001666666813</td>
</tr>
</tbody></table>
</li>
<li><p>符号微分</p>
<p>符号微分可以利用代数软件自动实现一些微分的公式，比如：<br>$$<br>\frac{d}{dx}(f(x)+g(x)) \leadsto \frac{d}{dx}f(x)+\frac{d}{dx}g(x)<br>$$</p>
<p>$$<br>\frac{d}{dx}(f(x)g(x))\leadsto(\frac{d}{dx}f(x))g(x)+f(x)(\frac{d}{dx}g(x))<br>$$</p>
<p>符号微分有一些不足之处：1）编译时间较长，特别是对于循环，需要很长时间进行编译；2）为了进行符号微分，一般需要设计一种专门的语言来表示数学表达式，并且要对变量（符号）进行预先声明；3）很难对程序进行调试。</p>
<p>同时，符号微分还存在表达式膨胀（ expression swell）的问题，如表所示：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/expression%20swell.png" style="zoom:50%;" />
</li>
<li><p>自动微分（Automatic Differentiation，AD）</p>
<p>以复合函数$f(x;w,b)$为例：<br>$$<br>f(x;w,b)&#x3D;\frac{1}{\exp(-(wx+b))+1}<br>$$<br>将其分解为一系列基本操作，并且构成一个计算图，计算图中的每个非叶子节点表示一个基本操作，每个叶子节点为一个输入变量或常量。下图给出了当 𝑥 &#x3D; 1, 𝑤 &#x3D; 0, 𝑏 &#x3D; 0 时复合函数$f(x;w,b)$的计算图：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0f%E7%9A%84%E8%AE%A1%E7%AE%97%E5%9B%BE.png" style="zoom:50%;" />

<p>可以看出该函数由6个基本函数组成，如下表所示：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E7%BB%84%E6%88%90f%E7%9A%84%E5%85%AD%E4%B8%AA%E5%9F%BA%E6%9C%AC%E5%87%BD%E6%95%B0.png" style="zoom:50%;" />

<p>通过计算图上与参数$w$和$b$有关的路径可以得到函数$f(x;w,b)$关于参数$w$和$b$的导数：</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/f%E5%85%B3%E4%BA%8Ew%E5%92%8Cb%E7%9A%84%E5%AF%BC%E6%95%B0.png" style="zoom:50%;" />

<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%AF%BC%E6%95%B0f%E4%BB%A3%E5%85%A5%E5%80%BC.png" style="zoom:50%;" />

<p>前向模式：与计算图计算方向相同递归地计算梯度，其过程如下：<br>$$<br>\begin{align*}<br>&amp;\frac{\partial h_1}{\partial w}&#x3D;x&#x3D;1    \<br>&amp;\frac{\partial h_2}{\partial w}&#x3D;\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial w}&#x3D;1 \times 1&#x3D;1 \<br>&amp;\frac{\partial h_3}{\partial w}&#x3D;\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial w}&#x3D;-1\times 1&#x3D;-1 \<br>&amp;\frac{\partial h_4}{\partial w}&#x3D;\frac{\partial h_4}{\partial h_3}\frac{\partial h_3}{\partial w}&#x3D;\exp(h3)\times -1&#x3D;1\times -1&#x3D;-1 \<br>&amp;\frac{\partial h_5}{\partial w}&#x3D;\frac{\partial h_5}{\partial h_4}\frac{\partial h_4}{\partial w}&#x3D;1\times-1&#x3D;-1\<br>&amp;\frac{\partial h_6}{\partial w}&#x3D;\frac{\partial h_6}{\partial h_5}\frac{\partial h_5}{\partial w}&#x3D;-0.25\times-1&#x3D;0.25\<br>&amp;\frac{\partial f(x;w,b)}{\partial w}&#x3D;\frac{\partial f(x;w,b)}{\partial h_6}\frac{\partial h_6}{\partial w}&#x3D;1\times0.25&#x3D;0.25<br>\end{align*}<br>$$<br>反向模式：与反向传播的计算方式相同，可以说反向传播是反向模式的一种特殊形式。</p>
<img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%89%8D%E5%90%91%E6%A8%A1%E5%BC%8F%E5%92%8C%E5%8F%8D%E5%90%91%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AF%B9%E6%AF%94.png"/>

<p>如何理解？对于像神经网络这种模型，通常输入是上万到上百万维，而输出损失函数是一维的模型，只需要一遍反向模式的计算过程，便可以求出输出对于各个输入的导数，从而轻松求取梯度用于后续优化更新。</p>
</li>
</ul>
<h5 id="如何理解通用近似定理-如何理解神经网络-？"><a href="#如何理解通用近似定理-如何理解神经网络-？" class="headerlink" title="如何理解通用近似定理(如何理解神经网络)？"></a>如何理解通用近似定理(如何理解神经网络)？</h5><p>​    如图所示，现在有如下函数需要我们去拟合，显然这条红色的折线段可以表示为：红色线&#x3D;多个分段函数累加+常量（偏置）</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86.png"></p>
<p>​    如图所示，红色折线段可以由<strong>蓝色线段0（常量偏置）+蓝色线段1+蓝色线段2+蓝色线段3</strong>合成。</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%861.png"></p>
<p>​    通过这个例子我们可以知道任意的分段线性函数可以由一个常量+多个如图所示的蓝色函数组成，只是不同的分段线性函数所用的蓝色函数不一定相同。</p>
<p>​    当然实际我们遇到的函数困难如下图所示，是一条连续的曲线，我们依然可以用多条直线段合成去逼近这条曲线（只要我们绿色的点取的足够的多），这里也就告诉我们只要蓝色的函数足够地多，那么任意一条曲线我们都可以去逼近。</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%862.png"></p>
<p>​    那么现在的问题就是我们怎么去表示这些蓝色的函数，如下图所示，如果用分段函数的形式去表示，结构有一些复杂，仔细观察，这样的函数非常像sigmoid函数，如下图所示：</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%863.png"></p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%864.png"></p>
<p>所以我们可以用形如：$y&#x3D;c\frac{1}{1+e^{-(b+wx_1)}}$的曲线（平滑，易于求导）去逼近上述蓝色的分段函数（这里我们称之为“Hard Sigmoid”），现在我们的目标就是用sigmoid函数去逼近各种各样的蓝色分段函数。</p>
<p>​    此时我们只需要调整参数值，就可以得到不同的sigmoid函数，总结一下，整体的流程就是：通过各种的sigmoid函数去逼近各种各样的hard sigmoid(蓝色函数)，通过多个hard sigmoid合成得到线性分段函数，用线性分段函数去近似各种的连续函数。</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%865.png"></p>
<p>​    来看刚刚的例子，如图所示，这条红色的线性分段函数便可以用如下的形式去拟合：<br>$$<br>y&#x3D;b+\sum_ic_isigmoid(b_i+wix_1)<br>$$<br><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%866.png"></p>
<p>这里出现了<strong>通用近似定理</strong>的雏形，想对于之前的只有一个特征的线性回归模型$y&#x3D;b+wx_1$，此处的模型变得更有弹性。如果线性模型变为多维特征输入，形如：<br>$$<br>y&#x3D;b+\sum_jw_jx_j<br>$$<br>我们同样可以用如下的模型去拟合：<br>$$<br>y&#x3D;b+\sum_ic_isigmoid(bi+\sum_jw_{ij}x_j)<br>$$<br>这个就是通用近似定理给出的形式。上面的公式拆解一下，就变成了神经网络的形式。现在我们只关注$sigmoid(bi+\sum_jw_{ij}x_j)$，我们假定i&#x3D;1，2，3；j&#x3D;1,2,3，可以有<br>$$<br>\begin{align*}<br>&amp;r_1&#x3D;b_1+w_{11}x_1+w_{12}x_2+w_{13}x_3\<br>&amp;r_2&#x3D;b_2+w_{21}x_1+w_{22}x_2+w_{23}x_3\<br>&amp;r_3&#x3D;b_3+w_{31}x_1+w_{32}x_2+w_{33}x_3\<br>\end{align*}<br>$$<br>也就是：<br>$$<br>\left[\begin{matrix}<br>r_1\<br>r_2\<br>r_3<br>\end{matrix}\right]&#x3D;<br>\left[\begin{matrix}<br>b_1\<br>b_2\<br>b_3<br>\end{matrix}\right]+<br>\left[\begin{matrix}<br>w_{11}&amp;w_{12}&amp;w_{13}\<br>w_{21}&amp;w_{22}&amp;w_{23}\<br>w_{31}&amp;w_{32}&amp;w_{33}\<br>\end{matrix}\right]<br>\left[\begin{matrix}<br>x_1\<br>x_2\<br>x_3<br>\end{matrix}\right]<br>$$<br>记作：<br>$$<br>\mathbf{r&#x3D;b+wx}<br>$$<br>最终函数的形式也就可以表示为:<br>$$<br>y&#x3D;b+\mathbf{c^T}\sigma(\mathbf{b+wx})<br>$$<br>这便是一个神经网络的结构，如下图所示，这里也解释了神经网络是如何拟合任意函数的。</p>
<p><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%867.png"></p>
<p>​    简单地说，通用近似定理定义了一个神经网络，只需要其中有一个包含足够多但有限数量神经元的隐藏层，在激活函数的作用下（书中说𝜙(⋅)是一个非常数、有界、单调递增的连续函数），我的理解是就是一个S型的函数，后面由<em>Kurt</em> <em>Hornik</em>证明通用近似定理并不依赖于特定的激活函数，而是由多层前馈网络结构所决定的，也就是说该定理适合所有激活函数，比如Relu函数，其实两个Relu函数便可以逼近sigmoid函数。</p>
<p>​    这里有一个有趣的问题就是，既然只有一个隐藏层的浅层神经网络可以拟合出任意函数，为什么现在的神经网络更加追求称为一个很高的瘦子（即追求深度），其中的一个解释就是如果只有一个隐藏层，那么其中的参数量会非常多，容易发现过拟合，当然现在层数很多的深层神经网络，纵然是作者自己有些原因都无法解释，这也是为什么现在很多人称深度学习为”玄学”，这里面更多地原因还需要在以后的学习中去体会。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>《神经网络与深度学习》第4-5章</p><p><a href="http://caixindi.github.io/神经网络与深度学习/《神经网络与深度学习》第4-5章/">http://caixindi.github.io/神经网络与深度学习/《神经网络与深度学习》第4-5章/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Cindy</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-08-10</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络与深度学习</a></div><div class="sharethis-inline-share-buttons"></div><script src="//platform-api.sharethis.com/js/sharethis.js#property=5ab6f60ace89f00013641890&amp;product=inline-share-buttons" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC7%E7%AB%A0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">《神经网络与深度学习》第7章</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%AC2-3%E7%AB%A0/"><span class="level-item">《神经网络与深度学习》第2-3章</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/20220313221114.png" alt="Cindy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Cindy</p><p class="is-size-6 is-block">Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Wuhan</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">61</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">25</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/caixindi" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/caixindi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#如何理解反向传播？"><span class="level-left"><span class="level-item">1</span><span class="level-item">如何理解反向传播？</span></span></a></li><li><a class="level is-mobile" href="#自动梯度计算"><span class="level-left"><span class="level-item">2</span><span class="level-item">自动梯度计算</span></span></a></li><li><a class="level is-mobile" href="#如何理解通用近似定理-如何理解神经网络-？"><span class="level-left"><span class="level-item">3</span><span class="level-item">如何理解通用近似定理(如何理解神经网络)？</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/CS217/"><span class="level-start"><span class="level-item">CS217</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/EdgeX/"><span class="level-start"><span class="level-item">EdgeX</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/Git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Golang/"><span class="level-start"><span class="level-item">Golang</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/kubeedge/"><span class="level-start"><span class="level-item">kubeedge</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">智能计算系统</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"><span class="level-start"><span class="level-item">杂七杂八</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%A1%AC%E4%BB%B6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8A%A0%E9%80%9F/"><span class="level-start"><span class="level-item">硬件神经网络加速</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">神经网络与深度学习</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E4%B8%8E%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"><span class="level-start"><span class="level-item">计算机组成与体系结构</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/CS217-Lecture/"><span class="tag">CS217 Lecture</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EdgeX%E5%B9%B3%E5%8F%B0%E6%9E%84%E5%BB%BA/"><span class="tag">EdgeX平台构建</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EdgeX%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%A5%E5%8F%8A%E6%8E%A8%E7%90%86/"><span class="tag">EdgeX机器学习以及推理</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EdgeX%E6%9E%84%E5%BB%BA/"><span class="tag">EdgeX构建</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EdgeX%E6%A6%82%E5%BF%B5/"><span class="tag">EdgeX概念</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EdgeX%E9%83%A8%E7%BD%B2/"><span class="tag">EdgeX部署</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"><span class="tag">Git使用指南</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Golang%E5%9C%A3%E7%BB%8F/"><span class="tag">Golang圣经</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"><span class="tag">Java并发编程</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java%E7%9F%A5%E8%AF%86%E7%82%B9/"><span class="tag">Java知识点</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux%E7%9F%A5%E8%AF%86%E7%82%B9/"><span class="tag">Linux知识点</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Redis/"><span class="tag">Redis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker%E9%83%A8%E7%BD%B2/"><span class="tag">docker部署</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kubeedge/"><span class="tag">kubeedge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kubeedge-example/"><span class="tag">kubeedge example</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A5%BD%E7%9C%8B%E7%9A%84PowerShell/"><span class="tag">好看的PowerShell</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/"><span class="tag">快速傅里叶变换</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/"><span class="tag">智能计算系统学习</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9A%97%E7%A1%85/"><span class="tag">暗硅</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%83%8C%E6%99%AF%E4%BB%A5%E5%8F%8A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/"><span class="tag">硬件加速背景以及卷积神经网络优化</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">神经网络与深度学习</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%B2%97%E7%B2%92%E5%BA%A6%E5%8F%AF%E9%87%8D%E6%9E%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8EPlasticine/"><span class="tag">粗粒度可重构体系结构与Plasticine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E4%B8%8E%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"><span class="tag">计算机组成与体系结构</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8A%A0%E9%80%9F/"><span class="tag">轻量级神经网络加速</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E5%89%96%E6%9E%90/"><span class="tag">高性能矩阵乘法剖析</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/cindy.svg" alt="Cindy" height="28"></a><p class="is-size-7"><span>&copy; 2022 Cindy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Discuss on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus/discussions"><i class="fas fa-comments"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><div id="outdated"><h6>Your browser is out-of-date!</h6><p>Update your browser to view this website correctly.&amp;npsb;<a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p><p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">×</a></p></div><script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script><script>window.addEventListener("load", function () {
            outdatedBrowser({
                bgColor: '#f25648',
                color: '#ffffff',
                lowerThan: 'object-fit' // display on IE11 or below
            });
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>