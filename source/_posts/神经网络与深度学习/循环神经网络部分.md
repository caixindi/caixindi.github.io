---
title: 《神经网络与深度学习》循环随机网络部分
date: 2021-09-01
categories:
- 神经网络与深度学习
tags:
- 神经网络与深度学习
language: zh-CN
toc: true
---

#### 基于门控的循环神经网络

##### 长短期记忆网络(LSTM)

​	这个部分主要就是理解如下图所示的循环单元结构：![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/LSTM%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%E7%BB%93%E6%9E%84.png)

<!--more-->

图中有三个门，分别是输入门${i}_t$，遗忘门${f}_t$和输出门${o}_t$。其计算过程为：1）首先利用上一 时刻的外部状态${h}_{t-1}$ 和当前时刻的输入${x}_t$，计算出三个门，以及候选状态${\widetilde{c}}_t$；2） 结合遗忘门 ${f}_t$ 和输入门${i}_t$来更新记忆单元${c}_t$；3）结合输出门${o}_t$，将内部状态的信息传递给外部状态${h}_t$．（根据谷歌的测试表明，LSTM中对学习贡献较大的是Forget gate，其次是Input gate，最次是Output gate）

![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/LSTM%E5%85%AC%E5%BC%8F.png)

##### LSTM网络的各种变体

- 无遗忘门的LSTM网络

  内部状态更新方式为：
  $$
  c_t=c_{t-1}+i_{t}\odot\widetilde{c}_t
  $$

- peephole连接

  使三个门不但依赖于输入${x}_t$和上一时刻的隐状态${h}_{t-1}$，也依赖于上一个时刻的记忆单元${c}_{t-1}$。

- 耦合输入门和遗忘门 

  将LSTM网络中的输入门和遗忘门合并为一个门，内部状态更新方式为：
  $$
  c_t=(1-{i}_t)\odot{c}_{t-1}+i_{t}\odot\widetilde{c}_t
  $$

##### 门控循环单元网络(GRU)

​	如图所示：

![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/GRU.png)

​	GRU的输入输出结构和普通的RNN相同，都是一个当前的输入$x_t$和上一时刻传下来的隐状态$h_{t-1}$，通过GRU得到输出$y_t$和要传给下一时刻的隐状态$h_t$。

​	与LSTM不同的是，GRU只有两个门控，分别为重置门（Reset Gate）和更新门（Update Gate）。
$$
\pmb{r}_t=\sigma(\pmb{W}_r\pmb{x}_t+\pmb{U}_r\pmb{h}_{t-1}+\pmb{b}_r)
$$

$$
\pmb{z}_t=\sigma(\pmb{W}_z\pmb{x}_t+\pmb{U}_z\pmb{h}_{t-1}+\pmb{b}_z)
$$

​	GRU工作的整个流程就是：

​	（1）得到门控信号后，首先通过重置门控来得到重置之后的信息，即：
$$
\pmb{h}_{t-1}'=\pmb{h}_{t-1}\odot \pmb{r}_t
$$
再将$\pmb{h}_{t-1}'$与输入$\pmb{x}_t$拼接，再通过一个tanh激活函数将数据缩放到-1~1的范围，得到$\widetilde{\pmb{h}}_t$，即：
$$
\widetilde{\pmb{h}}_t=\tanh(\pmb{W}_h\pmb{x}_t+\pmb{U}_h\pmb{h}_{t-1}'+\pmb{b}_h)
$$
​	（2）更新记忆：这个阶段我们同时继续了遗忘和记忆两个步骤。通过更新门来控制当前状态需要从历史状态中保留多少信息（不经过非线性变换），以及需要从候选状态中接受多少新信息，即：
$$
\pmb{h}_t=\pmb{z}_t\odot \pmb{h}_{t-1}+(1-\pmb{z}_t)\odot \widetilde{\pmb{h}}_t
$$
这里便是与LSTM不一样的地方，LSTM使用两个门控来进行遗忘和记忆，而GRU只需要一个门控即可完成。不难看出，当$\pmb{z}_t=0,\pmb{r}_t=1$时，GRU网络退化为简单循环网络；若$\pmb{z}_t=0,\pmb{r}_t=0$ 时，当前状态$\pmb{h}_t$只和当前输入$\pmb{x}_t$相关，和历史状态$\pmb{h}_{t-1}$无关．当$\pmb{z}_t=1$时，当前状态$\pmb{h}_t=\pmb{h}_{t-1}$等于上一时刻状态$\pmb{h}_{t-1}$，和当前输入$\pmb{x}_t$无关。

​	由于GRU与LSTM的实验效果相似，但是GRU的参数量比LSTM更少，所以一般情况下模型都会选择GRU，这样收敛速度更快，可以快速迭代。

#### 深层循环神经网络

##### 堆叠循环神经网络

​	将多个循环网络堆叠起来，从不同时刻的角度来看，这个神经网络是很深的，从同一时刻输入到输出的角度来看，这个神经网络又是很浅的，如图所示：

![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E6%8C%89%E6%97%B6%E9%97%B4%E5%B1%95%E5%BC%80%E7%9A%84%E5%A0%86%E5%8F%A0%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

##### 双向循环神经网络

​	如图所示，有时间顺序和时间逆序两个序列，某一个时刻的输出不但和过去时刻的信息有关，也和后续时刻的信息有关。

![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

#### 扩展到图结构

##### 递归神经网络

​	如图所示：

![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

​	递归神经网络就是一个树状的参差结构，当递归神经网络的每个父节点都仅与一个子节点连接时，就退化成一个简单循环神经网络。

##### 图神经网络

[A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/pdf/1901.00596.pdf)

- 图卷积网络（Graph Convolution Networks，GCN）
  - 基于谱（Spectral-based）
  - 基于空间（spatial-based）
- 图注意力网络（Graph Attention Networks）
- 图自编码器（ Graph Autoencoders）
- 图生成网络（ Graph Generative Networks） 
- 图时空网络（Graph Spatial-temporal Networks）

