---
title: 硬件加速背景以及卷积神经网络优化
date: 2021-12-03
categories:
- 硬件神经网络加速
tags:
- 硬件加速背景以及卷积神经网络优化
language: zh-CN
toc: true
---

	大数据时代的到来，数据呈爆发式增长的态势，深度学习技术不断发展，比如图像识别、语音识别和自然语言处理等。但是这些深度学习技术都有着极为庞大的计算量，对芯片的性能功耗要求很高，现今，比较热门的比如神经网络被广泛应用于人工智能应用之中，而传统的通用芯片再处理复杂神经网络的时候收到了带宽和能耗的限制，因此就推动了人们对深度学习硬件加速器的研究，目前主流的硬件加速器有三类：GPU、ASIC和FPGA。

- GPU：与传统CPU不同，GPU的内部拥有大量的逻辑计算单元，远超其中的控制单元和寄存器的规模；GPU拥有一些存储单元可以使得GPU线程之间可以共享这些内存而不依赖于全局内存；GPU拥有相对高速且内存带宽相对较大的全局内存。

- ASIC：针对某一个或者某一类算法进行硬件定制，通常来说，相对于其他硬件加速器，ASIC加速深度学习算法能取得较高的性能和功耗，但是其开发周期长，成本高，缺乏灵活性。

- FPGA：与其他硬件加速其不同的是FPGA具有可编程性，也就是FPGA内部的逻辑块是可以通过编程进行调整的，因此灵活性较高，并且也拥有不错的性能和较低的功耗。现阶段，GPU更适合深度学习算法的训练阶段，FPGA更适合深度学习算法的推理阶段。

  <!--more-->

​    当前还有许多关于神经网络专用加速器的研究，一类是基于冯诺依曼体系结构下的加速器，其设计的重点就是如何平衡片上片外的数据分配，最小化数据的搬移。在运算单元结构设计中，如DianNao，DaDianNao采用了NFU结构作为加速器的基本处理单元，很好地支持了神经网络的计算。又有Google TPU所采用的脉动整列结构，让数据在运算单元的整列中流动了起来，增加了数据的复用，减少了访存次数；在存储结构的设计中，比如分块存储，双缓冲等，又因为传统2D存储结构的DDR技术的带宽已经不能适应现在神经网络的规模，人们将3D存储技术也加入到了神经网络加速器的设计之中。另一类是采用忆阻器等新器件来设计处理存储一体化的加速器。它本身具有存储数据的功能，另外利用基尔霍夫定律产生的位线电流就是卷积运算乘累加的结果，节省了乘法和累加的计算时间。

​	前面讲了体系结构方面专用芯片对神经网络的支持，在数据流调度的过程中，神经网络中常用的数据流有权重固定流、输出固定流、无局部复用和行固定流，常用的数据流优化处理手段有0值跳过、稀疏矩阵、参数压缩等。

​	接下来介绍卷积神经网络，卷积神经网络由于其大部分的计算都是卷积操作，而卷积层包含的是大量的乘加运算，因此利用FPGA加速神经网络的工作性能都受限于片上数字处理单元（DSP）的数量。因此如何提高DSP的利用率，有如下四种实现卷积操作的算法：

- 非快速算法

  - 空间卷积算法：易于实现，可以很快地部署到CPU、GPU上执行，计算公式为：
    $$
    Out(o,p,q)=\sum_{c<C,i<K,j<L}F(o,c,i,j)\times In(c,p\times st_r+i-pad,q\times st_c+j-pad)
    $$
    其中In为输入数据（三维张量），其第一维是通道数 C, 第二维是图片高度 H, 第三维是图片宽度 W。F 对应着卷积层中的 Filter（四维），O 表示 输出通道数, C 表示输入通道数, K 为 Filter 高度, L 为 Filter 宽度。另外的输入 str,stc, pad 分别为 Filter 的沿高度方向滑动距离与沿宽度方向滑动距离以及边界处补全长度。

  - 通用矩阵乘法算法（GEMM）：主要就是利用现今成熟的矩阵乘法程序完成卷积操作，前面的学习中有涉及过，此处不作赘述。

- 快速算法：非快速算法中输出特征图中的每个元素被单独计算，而快速算法在运算过程中对输入的局部元素同时计算。快速算法可分为三个步骤：（1）输入块与卷积核变换（2）点对点乘法（3）逆变换

  - [FFT算法](https://blog.csdn.net/WADuan2/article/details/79529900):可以使运算复杂度从 O(N²) 减少到 O(NlogN) 的复数乘法, 且不会出现精度损失。

  - Winograd算法

​    如今卷积神经网络的规模日益庞大, 而大量应用场景都无法提供相应的必需资源，现在也有许多卷积神经网络结构优化的研究。

- 网络剪枝与稀疏化

  下表是不同网络剪枝后的效果，不难发现网络剪枝极大地减少了参数量，并且对精度的影响微乎其微。

  ![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D%E5%AF%B9%E4%B8%8D%E5%90%8C%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8E%8B%E7%BC%A9%E6%95%88%E6%9E%9C.png)

- 张量分解

  将原始张量分解为若干低秩张量，从而减少卷积操作数量。然而目前大多数的张量分解方法都是逐层分解网络，缺乏整体性的考虑, 有可能导致不同隐含层之间的信息损失。并且涉及矩阵分解操作，需要花费高昂的计算资源。

- 知识迁移

  过程如图所示，简单来说就是将没有标签的数据经过教师网络进行预测，人工合成为有标签的数据交给学生网络进行训练。这样就可以使得学生网络具有与教师网络相当的性能，但是参数数量大幅降低。

  ![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB%E8%BF%87%E7%A8%8B-163428749016319.png)

- 精细模块设计

  - Inception模块

    如将5×5卷积分解为两个3×3卷积，下图可以证明替换的有效性，并且只需原来花费（3\*3+3\*3）/5*5=72%的计算开销。

    ![](https://cxd-note-img.oss-cn-hangzhou.aliyuncs.com/typora-note-img/%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB%E8%BF%87%E7%A8%8B-163428749016319.png)

    1×1卷积可以减少神经网络的参数量，还可以压缩通道数，提高计算效率。

  - 网中网（Network in network）

    一种区别于广义线性模型的非线性结构—Mlpconv, 即在卷积核后面添加一个多层感知机。增强了网络对局部感知野的特征辨识能力和非线性表达能力。通过堆叠Mlpconv层构建出的网络被称为网中网。

  - 残差模块

    与高速网络不同，残差网络的门限机制不是可学习的，而是恒定的，始终保持信息畅通状态，大幅降低了网络复杂度，加速了网络训练速度。