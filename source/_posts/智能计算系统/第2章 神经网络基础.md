###  第2章 神经网络基础

引：神经网络是一种机器学习算法

#### 2.1 从机器学习到神经网络

##### 2.1.1 基本概念

​		人工智能、机器学习、神经网络、深度学习之间的关系：<img src="../img/第2章 神经网络基础/人工智能、机器学习、神经网络、深度学习之间的关系.png" alt="QQ截图20210409220014" style="zoom:67%;" />

<font color='blue'>机器学习</font>：计算机通过不断地从经验或数据中学习来逐步提升智能处理能力。

<font color='red'>典型的机器学习过程</font><img src="../img/第2章 神经网络基础/典型的机器学习过程-163428755731520.png" alt="image" style="zoom:80%;" />

常用符号说明：

|   定义   |     符号      |
| :------: | :-----------: |
| 输入数据 |       x       |
|  真实值  |       y       |
|  预测值  |   $\hat{y}$   |
| 模型函数 |     H(X)      |
| 激活函数 |     G(X)      |
| 损失函数 |     L(X)      |
|   标量   |   *a、b、c*   |
|   向量   | ***a、b、c*** |
|   矩阵   | ***A、B、C*** |

##### 2.1.2 线性回归

​		线性回归是一种最简单的机器学习方法，线性回归不属于神经网络。

<font color='blue'>一元线性回归模型</font> $H_w(x_1)=w_0+w_1x_1$

<font color='blue'>二元线性回归模型</font> $H_w(x_1,x_2)=w_0+w_1x_1+w_2x_2$

<font color='blue'>多元线性回归模型</font> $H_w(x)=\sum_{i=0}^{n}w_ix_i=\hat{w}^Tx,x_0=1$

<img src="../img/第2章 神经网络基础/线性函数拟合.png" alt="线性函数拟合" style="zoom:50%;" />

------

方法：迭代法（梯度下降法）寻找参数

初始先给定一个$\hat{w}$,如0向量或者随机向量，让其沿着梯度下降的方向进行迭代，使得更新后的损失函数$L(\hat{w})$不断变小

$\hat{w}=\hat{w}-\alpha\frac{\partial L(\hat{w})}{\partial{\hat{w}}}$,其中$\alpha$称为学习率或者步长

迭代至找到使得$L(\hat{w})$最小的$\hat{w}$停止，从而得到回归模型参数

------

##### 2.1.3 感知机（Perceptron）模型

​		一个最简单的神经网络，可以完成简单的线性分类任务，感知机模型的训练目的是要找到一个超平面**S**($w^T+b=0$)，将线性可分的数据集T中的所有样本点正确地划分为两类。其中超平面是N维线性空间中维度为N-1的子空间，例如二维空间的超平面就是一条直线，三维空间的超平面就是一个二维平面。

<img src="../img/第2章 神经网络基础/2.1.3-1.png" alt="image-20210410003319551" style="zoom:50%;" />

<img src="../img/第2章 神经网络基础/2.1.3-2.png" alt="image-20210410003449833" style="zoom:50%;" />      <img src="../img/第2章 神经网络基础/2.1.3-3.png" alt="image-20210410003757917" style="zoom:50%;" />

##### 2.1.4 多层感知机

​		多层感知机（Multi-Layer Perceptron，MLP），由一组输入、一个隐层和一个输出层组成。只有一个隐层的多层感知机是最经典的浅层神经网络。浅层神经网络的问题是结构太简单，对复杂函数的表示能力非常有限，虽然有理论可以证明只有一个隐层的神经网络足以拟合出任意的函数，但是只有一个隐层的神经网络拟合出的函数可能会有很大的误差并且每一层需要的神经元数量可能非常多。

##### 2.1.5 深度学习（深层神经网络）

​		深层神经网络的隐层可以超过一层，现在的神经网络已经拥有上百层甚至上千层。深度学习的工作原理是通过对信息的多层抽取和加工来完成复杂的功能，其之所以能够成熟壮大得益于ABC三方面的影响：A是Algorithm（算法）,B是Big data（大数据）,C是Computing（算力）。

#### 2.2 神经网络训练

​		神经网络训练是通过调整隐层和输出层的参数，使得神经网络计算出来的结果$\hat{y}$与真实结果$y$尽量接近。神经网络的训练主要包括<font color='blue'>正向传播</font>和<font color='blue'>反向传播</font>两个过程。

##### 2.2.1 正向传播

​		正向传播的基本原理是，基于训练好的神经网络模型，输入目标通过权重、偏置和激活函数计算出隐层，隐层通过下一层的权重、偏置和激活函数计算出下一个隐层，通过逐层迭代，将输入的特征向量从低级特征逐步提取为抽象特征，最终输入目标结果。

##### 2.2.2 反向传播

​		反向传播的基本原理是，首先根据正向传播的结果和真实结果计算出损失函数$L(W)$，然后采用梯度下降法，通过链式求导法则计算出损失函数对每个权重和偏置的偏导，即权重或偏置对损失的影响，最后更新权重和偏置。总而言之，反向传播就是要将神经网络的输出误差，一级一级地传播到神经网络的输入，在这个过程中需要计算每一个参数$w$对总的损失函数的影响，即损失函数对每个$w$的偏导，通过不断迭代更新$w$的值，从而缩小输出值与真实值之间的误差。

#### 2.3 神经网络设计原则

##### 2.3.1 网络的拓扑结构

​		神经网络的结构包括输入、隐层和输出层。通常情况下，当给定训练样本以后，输入层和输出层的结点个数就已经确定了，但是隐层的层数以及隐层神经元的个数是可以动态调整的。神经网络中的隐层是用来提取输入特征中的隐藏规律的，隐层的结点个数对信息的提取能力影响很大，结点个数少了则神经网络提取特征信息的能力会很差，结点多了，就会出现过拟合的现象，导致神经网络泛化能力变差。因此在实践中需要反复调节隐层神经元的个数以及隐层的数量。

##### 2.3.2 激活函数

​		如果没有激活函数，神经元网络将无法解决非线性可分问题，因此激活函数对神经网络的影响很大，常见的激活函数有sigmoid函数、tanh函数、ReLU函数、PReLU/Leaky ReLU函数、ELU函数等。

###### 2.3.2.1 sigmoid函数

​		sigmoid函数也称S型生长曲线，可以将变量映射到0到1之间，其数学表达式为：$\sigma(x)=\frac{1}{1+e^{-x}}$

​		优点：平滑，易于求导。
​		缺点：输出的均值不是0，会导致下一层输入的均值产生偏移，可能会影响神经网络的收敛性；激活函数含有指数运算，计算量大，并且反向传播求误差梯度时，求导涉及除法；反向传播时，在趋向无穷的地方，函数值变化很小，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。

###### 2.3.2.2 tanh函数

​		tanh函数的数学定义为：
$$
tanh(x)=\frac{sinh(x)}{cosh(x)}=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=2\sigma(2x)-1
$$
​		tanh函数解决了sigmoid输出均值不为0的问题，其图像是中心对称的，但依然没有解决梯度消失的问题。

###### 2.3.2.3 ReLU函数

​		ReLU（Rectified Linear Unit，修正线性单元）函数首次应用于受限玻尔兹曼机。当输入是负数时，ReLU函数的输出为0；否则输出等于输入。其形式化定义为：
$$
f(x)=max(0,x)
$$
​		优点：ReLU计算简单，没有上述两种函数中的指数运算，可以用一条计算机实现，并且，当x>0时，ReLU函数可以保持梯度不衰减，从而缓解梯度消失的问题。

​		缺点：输出的均值不是0；对于某些训练样本，在训练的过程中会出现ReLU死掉的现象。在反向传播的过程中，如果学习率过大，可能会导致更新之后的偏置和权重是负数，进而导致下一轮正向传播的过程中ReLU的输入是负数，输出为0，在后续反向传播迭代的过程中，该处的梯度将会一直为0，相关参数的值也不会再改变，输出始终为0，出现ReLU死掉的现象；输出的范围是无限的，可能会导致神经网络的输出的幅值随着网络层数的增加而不断变大。

###### 2.3.2.4 PReLU/Leaky ReLU函数

​		PReLU/Leaky ReLU函数解决当x<0时ReLU函数死掉的问题，Leaky ReLU函数的形式化定义为：
$$
f(x)=max(\alpha x,x)
$$
​		其中参数$\alpha$是一个很小的常量，PReLU函数与Leaky ReLU函数类似，唯一的区别就是$\alpha$是可变参数。

###### 2.3.2.5 ELU函数

​		ELU（Exponential Linear Unit，指数线性单元）函数结合了sigmoid和ReLU函数，其定义为：
$$
f(x)=\begin{cases}
		x & {x>0}\\
		\alpha(e^{x}-1)&x\leqslant0
	\end{cases}
$$
​		其中$\alpha$为可调参数，ELU的输出均值接近0，加快了收敛速度，并且解决了梯度消失以及神经元死掉的问题，不足之处就是ELU函数涉及指数运算，计算复杂度比较高。

##### 2.3.3 损失函数

###### 2.3.3.1 均方差损失函数

​		
$$
J(\theta) = \frac{1}{2m}\sum_{i=0}^m(y^i - h_\theta(x^i))^2
$$
​		损失函数对各个参数$\theta$的梯度：
$$
\frac{\partial J(\theta)}{\partial\theta_j} = -\frac1m\sum_{i=0}^m(y^i - 	h_\theta(x^i))x^i_j
$$

###### 2.3.3.2 交叉熵损失函数

​		由于均方差损失函数和sigmoid函数的组合会出现梯度消失的现象，采用交叉熵损失函数与sigmoid函数组合可以避免这一现象。交叉熵损失函数的定义为：
$$
L=-\frac{1}{m}\sum_{x\in D}\sum_iy_i\ln(\hat{y_i})
$$
其中，m为训练集D中样本的总数量，i为分类类别。对于单标记分类问题，交叉熵可以简化为$L=-\frac{1}{m}\sum\limits_{x\in D}y_i\ln(\hat{y_i})$；对于多标记分类问题，可以转化为二分类问题，使用sigmoid激活函数时的交叉熵损失函数为
$$
L=-\frac{1}{m}\sum_{x\in D}(y\ln(\hat{y})+(1-y)\ln(1-\hat{y}))
$$

#### 2.4 过拟合与正则化

##### 2.4.1 过拟合

​		神经网络在学习的过程中，会出现在训练集中精度很高，在测试集中精度很差的情况，通常需要考虑神经网络是不是过拟合了，尤其是神经网络层数多、参数多时，此时神经网络泛化的能力比较差，可以使用许多不同形式的正则化方法，包括参数范数惩罚、稀疏化、Bagging集成、Dropout、提前终止、多任务学习、数据集增强、参数共享等。

##### 2.4.2 正则化

###### 2.4.2.1 参数范数惩罚

​		在损失函数中增加对高次项的惩罚，可以避免过拟合，正则化就是在损失函数中对不想要的部分加入惩罚项：
$$
\widetilde L(w;x,y)=L(w;x,y)+\theta\sum_{j=1}^kw_j^2
$$
其中$\theta$为正则化参数，对于神经网络来说，模型参数包括权重$\omega$和偏置值b，正则化过程一般对权重$\omega$进行惩罚，记$\Omega(\omega)$为正则化项，目标函数记为：
$$
\widetilde L(w;x,y)=L(w;x,y)+\theta\Omega(w)
$$
（1）$L^2$正则化

​		$L^2$正则化后的目标函数：
$$
\widetilde L(w;x,y)=L(w;x,y)+\frac{\theta}{2}\|w\|^2
$$
（2）$L^1$正则化

​		$L^1$正则化后的目标函数：
$$
\widetilde L(w;x,y)=L(w;x,y)+\theta\|w\|_1,\|w\|_1=\sum_i|w_i|
$$

###### 2.4.2.2 稀疏化

​		由于GPU的性能的提高已经逐步地在逼近物理极限，未来可能计算能力无法满足需求，采用稀疏化训练时可以人神经网络中的90%权重或神经元为0，从而大幅度降低在正向传播中的计算量。

###### 2.4.2.3 Bagging集成学习

​		通过训练不同的模型来共同决策测试样例的输出。

###### 2.4.2.4 Dropout

​		在训练时随机删掉一些隐层的节点，在计算的时候忽略掉这些连接，从而降低神经网络的复杂度，避免过拟合。

#### 2.5 交叉验证

​		交叉验证（cross-validation）是一种好的衡量机器学习模型的统计分析方法，可以有效避免划分训练集和测试集时的随机性对评价结果造成的影响。常用的有留一法交叉验证和K-折叠交叉验证。把原始数据集平均分为K组不重复的子集，每次选 K-1组子集作为训练集，剩下的一组子集作为验证集。这样可以进行K次试验并得到K个 模型，将这K个模型在各自验证集上的错误率的平均作为分类器的评价。相比留一法，其计算量更低，耗时更短。