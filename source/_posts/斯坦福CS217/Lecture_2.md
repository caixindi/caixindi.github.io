##### 机器学习简介

​		本节主要学习一些机器学习算法，部分算法研究在《智能计算系统》的学习中有所接触，还学习机器学习相关技术和常见的计算模式，从而帮助加速器设计人员高效地将这些模式应用于硬件。

##### 机器学习算法

- 回归

  求函数$f$使得对$\forall(x,y)\in(X,Y),f(x)\backsimeq y$。

- 线性回归

  求$w,b$使得$\forall(x,y)\in(X,Y),f(x)= wx+b\backsimeq y$。通过最小化损失函数得到$w,b$。

  损失函数：
  $$
  J（w,b）=\frac{1}{2}\sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})^2
  $$

- 逻辑回归

  用于分类问题，将线性回归的结果通过激活函数比如sigmoid函数$\sigma$:
  $$
  \sigma(z)=\frac{1}{1+e^{-z}}
  $$
  即求$w,b$使得$\forall(x,y)\in(X,Y),f(x)= \sigma(wx+b)\backsimeq y$

  损失函数：
  $$
  J（w,b）=-\sum_{i=1}^{m}(y^{(i)}\lg(\hat{y}^{(i)})+(1-y^{(i)})\lg(1-\hat{y}^{(i)}))
  $$

- 支持向量机

  找到一个超平面，以“最佳间隔”分隔数据。即：
  $$
  \hat{y}=g(wx+b)=+1,wx+b\geq0;-1,wx+b\leq0
  $$
  使边距最大化问题转化为优化问题：最大化$\frac{1}{\Vert w\Vert}$，也就是最小化$\frac{1}{2}\Vert w\Vert^2$使得$\forall i\in[1,m],y^{(i)}(wx^{(i)}+b)\geq1$,利用拉格朗日对偶得到最小化函数：
  $$
  J(w,b)=\sum_{i=1}^{m}max(0,1-y^{(i)}(wx^{(i)}+b))+R(w)
  $$

- 核技巧

  当数据不是线性可分时，将其映射到更高的维度，然后找到一个超平面将其线性分离。

- 全连接网络

  推断过程只需使用前向传播，训练过程需要同时使用前向传播和反向传播。反向传播代价很大因为涉及梯度计算。

- 机器学习常用模式

  - 都是优化问题
  - 都会存在一个需要最小化的损失函数
  - 都可以使用梯度下降之类的迭代算法解决最优化问题

  不是所有的机器学习算法都需要表示为优化问题，比如K-近邻算法。

  在推理和训练期间主要涉及的计算：算术运算（向量乘法、加法运算）和激活函数（ReLu、sigmoid 等）计算。以及训练时额外需要在迭代中计算梯度。

